import streamlit as st
import google.genai as genai
import requests
import json
import re
from unidecode import unidecode
from datetime import datetime

# ======================================
# CONFIG GEMINI (SDK má»›i)
# ======================================
# Make sure you set st.secrets["gemini_key"] in Streamlit Cloud
client = genai.Client(
    api_key=st.secrets["gemini_key"],
)

# ======================================
# DATA FILES
# ======================================
DATA_FILE = "data_tayninh.txt"
IMAGES_FILE = "images.json"

# Load dá»¯ liá»‡u áº£nh
try:
    with open(IMAGES_FILE, "r", encoding="utf-8") as f:
        images = json.load(f)
except Exception:
    images = {}
    st.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y images.json hoáº·c JSON khÃ´ng há»£p lá»‡")

# Load dá»¯ liá»‡u du lá»‹ch
try:
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        raw_text = f.read()
except Exception:
    raw_text = ""
    st.error("âŒ KhÃ´ng tÃ¬m tháº¥y file data_tayninh.txt")

# Chia dá»¯ liá»‡u theo Ä‘á»‹a Ä‘iá»ƒm (### TÃªn Ä‘á»‹a Ä‘iá»ƒm)
tourism_data = {}
current_key = None
for line in raw_text.splitlines():
    line = line.strip()
    if line.startswith("###"):
        place = line.replace("###", "").strip()
        tourism_data[place] = ""
        current_key = place
    elif current_key:
        tourism_data[current_key] += line + "\n"

# ======================================
# UTIL FUNCTIONS
# ======================================
def normalize(text):
    if not text:
        return ""
    t = unidecode(text.lower())
    t = re.sub(r"[^a-z0-9\s]", " ", t)
    return re.sub(r"\s+", " ", t).strip()

def is_new_question(user_msg, last_bot_msg):
    if not last_bot_msg:
        return True
    nm = normalize(user_msg)
    if len(nm.split()) <= 3:
        return False
    if any(x in nm for x in ["tai sao", "o dau", "gio mo cua", "la gi", "du lich", "bao nhieu"]):
        return True
    return False

@st.cache_data(ttl=300)
def get_weather_simple(lat, lon):
    url = (
        f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}"
        "&current_weather=true&hourly=precipitation_probability&timezone=auto"
    )
    try:
        res = requests.get(url, timeout=10)
        return res.json()
    except Exception:
        return None

# ======================================
# STREAMLIT UI
# ======================================
st.set_page_config(page_title="Chatbot Du Lá»‹ch TÃ¢y Ninh", page_icon="ğŸ—ºï¸")
st.title("ğŸ—ºï¸ Chatbot Du Lá»‹ch TÃ¢y Ninh â€“ Gemini Streaming")
st.caption("Made by ÄÄƒng Khoa ğŸ”° - 1.1")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_bot" not in st.session_state:
    st.session_state.last_bot = ""

# Show conversation
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("Nháº­p cÃ¢u há»i...")

if user_input:
    # 1. Hiá»ƒn thá»‹ tin nháº¯n User
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # 2. Bá» qua RAG (Táº M THá»œI) vÃ  táº¡o Prompt ÄÆ N GIáº¢N
    # LH: Loáº¡i bá» System Instruction nghiÃªm ngáº·t Ä‘á»ƒ kiá»ƒm tra
    lh = "Báº¡n lÃ  chatbot du lá»‹ch tá»‰nh TÃ¢y Ninh. Tráº£ lá»i ngáº¯n gá»n, chÃ­nh xÃ¡c, tiáº¿ng Viá»‡t."
    prompt_user = f"{lh}\n\nCÃ¢u há»i:\n{user_input}\n"
    
    # 4. Gá»i Gemini API (Sá»­a lá»—i Indentation vÃ  Logic)
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_text = ""
        
        # Cáº¥u hÃ¬nh Token Output (256 tokens)
        gemini_config = {"max_output_tokens": 256} 

        # --- Báº®T Äáº¦U Gá»ŒI API ---
        try:
            # A. Thá»­ Streaming
            stream = client.models.generate_content_stream(
                model="gemini-2.5-flash", 
                contents=prompt_user,
                config=gemini_config
            )

            for chunk in stream:
                chunk_text = ""
                try:
                    if hasattr(chunk, "text") and chunk.text:
                        chunk_text = chunk.text
                except Exception:
                    pass
                
                if chunk_text:
                    full_text += chunk_text
                    placeholder.markdown(full_text)

            if not full_text.strip():
                raise RuntimeError("Pháº£n há»“i rá»—ng (CÃ³ thá»ƒ bá»‹ lá»c ná»™i dung).") 

        except Exception as e_stream:
            # B. Náº¿u Stream lá»—i -> Fallback sang gá»i Sync
            try:
                resp = client.models.generate_content(
                    model="gemini-2.5-flash", 
                    contents=prompt_user,
                    config=gemini_config
                )
                
                # --- LOGIC Xá»¬ LÃ PHáº¢N Há»’I Ráº®N CHáº®C HÆ N (ÄÃƒ Sá»¬A Lá»–I THá»¤T Lá»€) ---
                full_text = ""
                
                # 1. KIá»‚M TRA Lá»–I Lá»ŒC AN TOÃ€N TRÆ¯á»šC
                if (hasattr(resp, "prompt_feedback") and resp.prompt_feedback is not None and 
                    hasattr(resp.prompt_feedback, "block_reason") and resp.prompt_feedback.block_reason):
                    
                    reason_name = resp.prompt_feedback.block_reason.name if hasattr(resp.prompt_feedback.block_reason, 'name') else 'LÃ½ do khÃ´ng xÃ¡c Ä‘á»‹nh'
                    full_text = f"ğŸš« Ná»™i dung bá»‹ cháº·n do vi pháº¡m chÃ­nh sÃ¡ch an toÃ n: **{reason_name}**"
                
                # 2. KIá»‚M TRA XEM CÃ“ TEXT TRáº¢ Vá»€ KHÃ”NG
                elif hasattr(resp, "text") and resp.text:
                    full_text = resp.text
                
                # 3. Náº¿u váº«n khÃ´ng cÃ³ ná»™i dung
                if not full_text:
                     full_text = "âš ï¸ Pháº£n há»“i rá»—ng hoáº·c khÃ´ng cÃ³ ná»™i dung liÃªn quan."

                placeholder.markdown(full_text)

            except Exception as e_sync:
                # C. Cáº£ 2 Ä‘á»u lá»—i -> In lá»—i chi tiáº¿t
                st.error("âŒ ÄÃ£ xáº£y ra lá»—i káº¿t ná»‘i Gemini:")
                st.write("Lá»—i Stream:", e_stream)
                st.write("Lá»—i Sync:", e_sync)
                st.stop()
        
        # --- Káº¾T THÃšC Gá»ŒI API ---

        # 5. LÆ°u lá»‹ch sá»­
        st.session_state.messages.append({"role": "assistant", "content": full_text})
        st.session_state.last_bot = full_text
        
        # 6. Hiá»ƒn thá»‹ áº£nh liÃªn quan (ÄÃ£ loáº¡i bá» logic RAG phá»©c táº¡p, chá»‰ giá»¯ láº¡i hiá»ƒn thá»‹)
        # Báº N Cáº¦N THÃŠM Láº I LOGIC TÃŒM KIáº¾M PLACE Táº I ÄÃ‚Y Náº¾U MUá»N HIá»‚N THá»Š áº¢NH
        
    # 7. Hiá»ƒn thá»‹ thá»i tiáº¿t (ÄÃ£ sá»­a lá»—i thá»¥t lá»)
    st.divider()
    cols_weather = st.columns(2)
    lat, lon = 10.5359, 106.4137 # Tá»a Ä‘á»™ TÃ¢y Ninh
    weather = get_weather_simple(lat, lon)
    
    if weather:
        current = weather.get("current_weather", {})
        temp = current.get("temperature", "--")
        
        with cols_weather[0]:
            st.info(f"ğŸŒ¤ï¸ Nhiá»‡t Ä‘á»™ TÃ¢y Ninh: **{temp}Â°C**")
    if weather:
        current = weather.get("current_weather", {})
        temp = current.get("temperature", "--")
        
        with cols_weather[0]:
            st.info(f"ğŸŒ¤ï¸ Nhiá»‡t Ä‘á»™ TÃ¢y Ninh: **{temp}Â°C**")



















